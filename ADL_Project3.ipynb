{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEVwIJBGxjNq5gDvY2Px5e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srikarraju/ADL/blob/main/ADL_Project3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCQKy3kSVtHx"
      },
      "source": [
        "from keras.layers import Input,Conv2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jZkgjF9RlIA"
      },
      "source": [
        "def DeepCapsNet(input_shape,num_class,routing_iters):\n",
        "  x = Input(shape=input_shape)\n",
        "  l = x\n",
        "\n",
        "  l = Conv2D(128,(3,3),strides=1,activation='relu',padding='same')(l)\n",
        "  l = BatchNormalization()(l)\n",
        "  l = ConvertToCaps()(l)\n",
        "\n",
        "  l = Conv2DCaps(32,4,kernel_size =(3,3),strides=2,r_num=1,b_alphas=[1,1,1])(l)\n",
        "  l_skip = Conv2DCaps(32,4,kernel_size=(3,3),strides=1,r_num=1,b_alphas=[1,1,1])(l)\n",
        "  l = Conv2DCaps(32,4,kernel_size=(3,3),strides=1,r_num=1,b_alphas=[1,1,1])(l)\n",
        "  l = Conv2DCaps(32,4,kernel_size=(3,3),strides=1,r_num=1,b_alphas=[1,1,1])(l)\n",
        "  layers.Add()([l,l_skip])\n",
        "\n",
        "  l = Conv2DCaps(32,8,kernel_size =(3,3),strides=2,r_num=1,b_alphas=[1,1,1])(l)\n",
        "  l_skip = Conv2DCaps(32,8,kernel_size=(3,3),strides=1,r_num=1,b_alphas=[1,1,1])(l)\n",
        "  l = Conv2DCaps(32,8,kernel_size=(3,3),strides=1,r_num=1,b_alphas=[1,1,1])(l)\n",
        "  l = Conv2DCaps(32,8,kernel_size=(3,3),strides=1,r_num=1,b_alphas=[1,1,1])(l)\n",
        "  layers.Add()([l,l_skip])\n",
        "\n",
        "  l = Conv2DCaps(32,8,kernel_size =(3,3),strides=2,r_num=1,b_alphas=[1,1,1])(l)\n",
        "  l_skip = Conv2DCaps(32,8,kernel_size=(3,3),strides=1,r_num=1,b_alphas=[1,1,1])(l)\n",
        "  l = Conv2DCaps(32,8,kernel_size=(3,3),strides=1,r_num=1,b_alphas=[1,1,1])(l)\n",
        "  l = Conv2DCaps(32,8,kernel_size=(3,3),strides=1,r_num=1,b_alphas=[1,1,1])(l)\n",
        "  layers.Add()([l,l_skip])\n",
        "  l1 = l\n",
        "\n",
        "  l = Conv2DCaps(32, 8, kernel_size=(3, 3), strides=(2, 2), r_num=1, b_alphas=[1, 1, 1])(l)\n",
        "  l_skip = ConvCapsuleLayer3D(kernel_size=3, num_capsule=32, num_atoms=8, strides=1, padding='same', routings=3)(l)\n",
        "  l = Conv2DCaps(32, 8, kernel_size=(3, 3), strides=(1, 1), r_num=1, b_alphas=[1, 1, 1])(l)\n",
        "  l = Conv2DCaps(32, 8, kernel_size=(3, 3), strides=(1, 1), r_num=1, b_alphas=[1, 1, 1])(l)\n",
        "  l = layers.Add()([l, l_skip])\n",
        "  l2 = l\n",
        "\n",
        "  la = FlattenCaps()(l2)\n",
        "  lb = FlattenCaps()(l1)\n",
        "  l = layers.Concatenate(axis=-1)([la,lb])\n",
        "\n",
        "  digit_caps = CapsuleLayer(num_capsule=num_class, dim_capsule=32, routings=routing_iters, channels=0, name='digit_caps')(l)\n",
        "  l = CapsToScalars(name='capsnet')(digits_caps)\n",
        "\n",
        "  m_capsnet = models.Model(inputs=x, outputs=l, name='capsnet_model')\n",
        "\n",
        "  y = Input(shape=(n_class,))\n",
        "\n",
        "  masked_by_y = Mask_CID()([digits_caps, y])\n",
        "  masked = Mask_CID()(digits_caps)\n",
        "\n",
        "  # Decoder Network\n",
        "  decoder = models.Sequential(name='decoder')\n",
        "  decoder.add(Dense(input_dim=32, activation=\"relu\", output_dim=7 * 7 * 16))\n",
        "  decoder.add(Reshape((7, 7, 16)))\n",
        "  decoder.add(BatchNormalization(momentum=0.8))\n",
        "  decoder.add(Conv2DTranspose(64, 3, 3, subsample=(1, 1), border_mode='same'))\n",
        "  decoder.add(Conv2DTranspose(32, 3, 3, subsample=(2, 2), border_mode='same'))\n",
        "  decoder.add(Conv2DTranspose(16, 3, 3, subsample=(2, 2), border_mode='same'))\n",
        "  decoder.add(Conv2DTranspose(1, 3, 3, subsample=(1, 1), border_mode='same'))\n",
        "  decoder.add(Activation(\"relu\"))\n",
        "  decoder.add(Reshape(target_shape=(28, 28, 1), name='out_recon'))\n",
        "\n",
        "  train_model = models.Model([x, y], [m_capsnet.output, decoder(masked_by_y)])\n",
        "  eval_model = models.Model(x, [m_capsnet.output, decoder(masked)])\n",
        "  train_model.summary()\n",
        "\n",
        "  return train_model, eval_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORWi9vbw5kRA"
      },
      "source": [
        "def train(model, data, hard_training, args):\n",
        "    # unpacking the data\n",
        "    (x_train, y_train), (x_test, y_test) = data\n",
        "\n",
        "    model.compile(optimizer=optimizers.Adam(lr=args.lr), loss=[margin_loss, 'mse'], loss_weights=[1, 0.4], metrics={'capsnet': \"accuracy\"})\n",
        "\n",
        "    # Begin: Training with data augmentation\n",
        "    def train_generator(x, y, batch_size, shift_fraction=args.shift_fraction):\n",
        "        train_datagen = ImageDataGenerator(featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False,\n",
        "                                           samplewise_std_normalization=False, zca_whitening=False, zca_epsilon=1e-06, rotation_range=0.1,\n",
        "                                           width_shift_range=0.1, height_shift_range=0.1, shear_range=0.0,\n",
        "                                           zoom_range=0.1, channel_shift_range=0.0, fill_mode='nearest', cval=0.0, horizontal_flip=True,\n",
        "                                           vertical_flip=False, rescale=None, preprocessing_function=None,\n",
        "                                           data_format=None)  # shift up to 2 pixel for MNIST\n",
        "        train_datagen.fit(x)\n",
        "        generator = train_datagen.flow(x, y, batch_size=batch_size, shuffle=True)\n",
        "        while True:\n",
        "            x_batch, y_batch = generator.next()\n",
        "            yield ([x_batch, y_batch], [y_batch, x_batch])\n",
        "\n",
        "    model.fit_generator(generator=train_generator(x_train, y_train, args.batch_size, args.shift_fraction),\n",
        "                                 steps_per_epoch=int(y_train.shape[0] / args.batch_size), epochs=args.epochs,\n",
        "                                 validation_data=[[x_test, y_test], [y_test, x_test]], callbacks=[lr_decay, log, checkpoint1, checkpoint2],\n",
        "                                 initial_epoch=int(args.ep_num),\n",
        "                                 shuffle=True)\n",
        "\n",
        "    parallel_model.save(args.save_dir + '/trained_model_multi_gpu.h5')\n",
        "    model.save(args.save_dir + '/trained_model.h5')\n",
        "\n",
        "    return parallel_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DILuFDj59ZJ"
      },
      "source": [
        "class args:\n",
        "    numGPU = 1\n",
        "    epochs = 5\n",
        "    batch_size = 256\n",
        "    lr = 0.001\n",
        "    lr_decay = 0.96\n",
        "    lam_recon = 0.4\n",
        "    r = 3\n",
        "    routings = 3\n",
        "    shift_fraction = 0.1\n",
        "    debug = False\n",
        "    digit = 5\n",
        "    save_dir = 'model/FMNIST/13'\n",
        "    t = False\n",
        "    w = None\n",
        "    ep_num = 0\n",
        "    dataset = \"FMNIST\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m23gM3Y6EcD"
      },
      "source": [
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import layers, initializers, regularizers, constraints\n",
        "from keras.utils import conv_utils\n",
        "from keras.layers import InputSpec\n",
        "from keras.utils.conv_utils import conv_output_length\n",
        "from batchdot import own_batch_dot\n",
        "\n",
        "\n",
        "cf = K.image_data_format() == '..'\n",
        "useGPU = True\n",
        "\n",
        "\n",
        "def squeeze(s):\n",
        "    sq = K.sum(K.square(s), axis=-1, keepdims=True)\n",
        "    return (sq / (1 + sq)) * (s / K.sqrt(sq + K.epsilon()))\n",
        "\n",
        "\n",
        "class ConvertToCaps(layers.Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(ConvertToCaps, self).__init__(**kwargs)\n",
        "        # self.input_spec = InputSpec(min_ndim=2)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        output_shape = list(input_shape)\n",
        "        output_shape.insert(1 if cf else len(output_shape), 1)\n",
        "        return tuple(output_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return K.expand_dims(inputs, 1 if cf else -1)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'input_spec': 5\n",
        "        }\n",
        "        base_config = super(ConvertToCaps, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "class FlattenCaps(layers.Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(FlattenCaps, self).__init__(**kwargs)\n",
        "        self.input_spec = InputSpec(min_ndim=4)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if not all(input_shape[1:]):\n",
        "            raise ValueError('The shape of the input to \"FlattenCaps\" '\n",
        "                             'is not fully defined '\n",
        "                             '(got ' + str(input_shape[1:]) + '. '\n",
        "                             'Make sure to pass a complete \"input_shape\" '\n",
        "                             'or \"batch_input_shape\" argument to the first '\n",
        "                             'layer in your model.')\n",
        "        return (input_shape[0], np.prod(input_shape[1:-1]), input_shape[-1])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        shape = K.int_shape(inputs)\n",
        "        return K.reshape(inputs, (-1, np.prod(shape[1:-1]), shape[-1]))\n",
        "\n",
        "\n",
        "class CapsToScalars(layers.Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(CapsToScalars, self).__init__(**kwargs)\n",
        "        self.input_spec = InputSpec(min_ndim=3)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return K.sqrt(K.sum(K.square(inputs + K.epsilon()), axis=-1))\n",
        "\n",
        "\n",
        "class Conv2DCaps(layers.Layer):\n",
        "\n",
        "    def __init__(self, ch_j, n_j,\n",
        "                 kernel_size=(3, 3),\n",
        "                 strides=(1, 1),\n",
        "                 r_num=1,\n",
        "                 b_alphas=[8, 8, 8],\n",
        "                 padding='same',\n",
        "                 data_format='channels_last',\n",
        "                 dilation_rate=(1, 1),\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 **kwargs):\n",
        "        super(Conv2DCaps, self).__init__(**kwargs)\n",
        "        rank = 2\n",
        "        self.ch_j = ch_j  # Number of capsules in layer J\n",
        "        self.n_j = n_j  # Number of neurons in a capsule in J\n",
        "        self.kernel_size = conv_utils.normalize_tuple(kernel_size, rank, 'kernel_size')\n",
        "        self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')\n",
        "        self.r_num = r_num\n",
        "        self.b_alphas = b_alphas\n",
        "        self.padding = conv_utils.normalize_padding(padding)\n",
        "        #self.data_format = conv_utils.normalize_data_format(data_format)\n",
        "        self.data_format = K.normalize_data_format(data_format)\n",
        "        self.dilation_rate = (1, 1)\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.input_spec = InputSpec(ndim=rank + 3)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.h_i, self.w_i, self.ch_i, self.n_i = input_shape[1:5]\n",
        "\n",
        "        self.h_j, self.w_j = [conv_utils.conv_output_length(input_shape[i + 1],\n",
        "                                                            self.kernel_size[i],\n",
        "                                                            padding=self.padding,\n",
        "                                                            stride=self.strides[i],\n",
        "                                                            dilation=self.dilation_rate[i]) for i in (0, 1)]\n",
        "\n",
        "        self.ah_j, self.aw_j = [conv_utils.conv_output_length(input_shape[i + 1],\n",
        "                                                              self.kernel_size[i],\n",
        "                                                              padding=self.padding,\n",
        "                                                              stride=1,\n",
        "                                                              dilation=self.dilation_rate[i]) for i in (0, 1)]\n",
        "\n",
        "        self.w_shape = self.kernel_size + (self.ch_i, self.n_i,\n",
        "                                           self.ch_j, self.n_j)\n",
        "\n",
        "        self.w = self.add_weight(shape=self.w_shape,\n",
        "                                 initializer=self.kernel_initializer,\n",
        "                                 name='kernel',\n",
        "                                 regularizer=self.kernel_regularizer,\n",
        "                                 constraint=self.kernel_constraint)\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if self.r_num == 1:\n",
        "            # if there is no routing (and this is so when r_num is 1 and all c are equal)\n",
        "            # then this is a common convolution\n",
        "            outputs = K.conv2d(K.reshape(inputs, (-1, self.h_i, self.w_i,\n",
        "                                                  self.ch_i * self.n_i)),\n",
        "                               K.reshape(self.w, self.kernel_size +\n",
        "                                         (self.ch_i * self.n_i, self.ch_j * self.n_j)),\n",
        "                               data_format='channels_last',\n",
        "                               strides=self.strides,\n",
        "                               padding=self.padding,\n",
        "                               dilation_rate=self.dilation_rate)\n",
        "\n",
        "            outputs = squeeze(K.reshape(outputs, ((-1, self.h_j, self.w_j,\n",
        "                                                   self.ch_j, self.n_j))))\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.h_j, self.w_j, self.ch_j, self.n_j)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'ch_j': self.ch_j,\n",
        "            'n_j': self.n_j,\n",
        "            'kernel_size': self.kernel_size,\n",
        "            'strides': self.strides,\n",
        "            'b_alphas': self.b_alphas,\n",
        "            'padding': self.padding,\n",
        "            'data_format': self.data_format,\n",
        "            'dilation_rate': self.dilation_rate,\n",
        "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
        "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
        "            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n",
        "            'kernel_constraint': constraints.serialize(self.kernel_constraint)\n",
        "        }\n",
        "        base_config = super(Conv2DCaps, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "class Mask_CID(layers.Layer):\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if isinstance(inputs, list):\n",
        "            assert len(inputs) == 2\n",
        "            inputs, a = inputs\n",
        "            mask = K.argmax(a, 1)\n",
        "        else:\n",
        "            x = K.sqrt(K.sum(K.square(inputs), -1))\n",
        "            mask = K.argmax(x, 1)\n",
        "\n",
        "        increasing = tf.range(start=0, limit=tf.shape(inputs)[0], delta=1)\n",
        "        m = tf.stack([increasing, tf.cast(mask, tf.int32)], axis=1)\n",
        "        masked = tf.gather_nd(inputs, m)\n",
        "\n",
        "        return masked\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if isinstance(input_shape[0], tuple):  # true label provided\n",
        "            return tuple([None, input_shape[0][2]])\n",
        "        else:  # no true label provided\n",
        "            return tuple([None, input_shape[2]])\n",
        "\n",
        "\n",
        "class ConvCapsuleLayer3D(layers.Layer):\n",
        "\n",
        "    def __init__(self, kernel_size, num_capsule, num_atoms, strides=1, padding='valid', routings=3,\n",
        "                 kernel_initializer='he_normal', **kwargs):\n",
        "        super(ConvCapsuleLayer3D, self).__init__(**kwargs)\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_capsule = num_capsule\n",
        "        self.num_atoms = num_atoms\n",
        "        self.strides = strides\n",
        "        self.padding = padding\n",
        "        self.routings = routings\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 5, \"The input Tensor should have shape=[None, input_height, input_width,\" \\\n",
        "                                      \" input_num_capsule, input_num_atoms]\"\n",
        "        self.input_height = input_shape[1]\n",
        "        self.input_width = input_shape[2]\n",
        "        self.input_num_capsule = input_shape[3]\n",
        "        self.input_num_atoms = input_shape[4]\n",
        "\n",
        "        # Transform matrix\n",
        "        self.W = self.add_weight(shape=[self.input_num_atoms, self.kernel_size, self.kernel_size, 1, self.num_capsule * self.num_atoms],\n",
        "                                 initializer=self.kernel_initializer,\n",
        "                                 name='W')\n",
        "\n",
        "        self.b = self.add_weight(shape=[self.num_capsule, self.num_atoms, 1, 1],\n",
        "                                 initializer=initializers.constant(0.1),\n",
        "                                 name='b')\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, input_tensor, training=None):\n",
        "\n",
        "        input_transposed = tf.transpose(input_tensor, [0, 3, 4, 1, 2])\n",
        "        input_shape = K.shape(input_transposed)\n",
        "        input_tensor_reshaped = K.reshape(input_tensor, [input_shape[0], 1, self.input_num_capsule * self.input_num_atoms, self.input_height, self.input_width])\n",
        "\n",
        "        input_tensor_reshaped.set_shape((None, 1, self.input_num_capsule * self.input_num_atoms, self.input_height, self.input_width))\n",
        "\n",
        "        # conv = Conv3D(input_tensor_reshaped, self.W, (self.strides, self.strides),\n",
        "        #                 padding=self.padding, data_format='channels_first')\n",
        "\n",
        "        conv = K.conv3d(input_tensor_reshaped, self.W, strides=(self.input_num_atoms, self.strides, self.strides), padding=self.padding, data_format='channels_first')\n",
        "\n",
        "        votes_shape = K.shape(conv)\n",
        "        _, _, _, conv_height, conv_width = conv.get_shape()\n",
        "        conv = tf.transpose(conv, [0, 2, 1, 3, 4])\n",
        "        votes = K.reshape(conv, [input_shape[0], self.input_num_capsule, self.num_capsule, self.num_atoms, votes_shape[3], votes_shape[4]])\n",
        "        votes.set_shape((None, self.input_num_capsule, self.num_capsule, self.num_atoms, conv_height.value, conv_width.value))\n",
        "\n",
        "        logit_shape = K.stack([input_shape[0], self.input_num_capsule, self.num_capsule, votes_shape[3], votes_shape[4]])\n",
        "        biases_replicated = K.tile(self.b, [1, 1, conv_height.value, conv_width.value])\n",
        "\n",
        "        activations = update_routing(\n",
        "            votes=votes,\n",
        "            biases=biases_replicated,\n",
        "            logit_shape=logit_shape,\n",
        "            num_dims=6,\n",
        "            input_dim=self.input_num_capsule,\n",
        "            output_dim=self.num_capsule,\n",
        "            num_routing=self.routings)\n",
        "\n",
        "        a2 = tf.transpose(activations, [0, 3, 4, 1, 2])\n",
        "        return a2\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        space = input_shape[1:-2]\n",
        "        new_space = []\n",
        "        for i in range(len(space)):\n",
        "            new_dim = conv_output_length(space[i], self.kernel_size, padding=self.padding, stride=self.strides, dilation=1)\n",
        "            new_space.append(new_dim)\n",
        "\n",
        "        return (input_shape[0],) + tuple(new_space) + (self.num_capsule, self.num_atoms)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'kernel_size': self.kernel_size,\n",
        "            'num_capsule': self.num_capsule,\n",
        "            'num_atoms': self.num_atoms,\n",
        "            'strides': self.strides,\n",
        "            'padding': self.padding,\n",
        "            'routings': self.routings,\n",
        "            'kernel_initializer': initializers.serialize(self.kernel_initializer)\n",
        "        }\n",
        "        base_config = super(ConvCapsuleLayer3D, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "def update_routing(votes, biases, logit_shape, num_dims, input_dim, output_dim,\n",
        "                   num_routing):\n",
        "    if num_dims == 6:\n",
        "        votes_t_shape = [3, 0, 1, 2, 4, 5]\n",
        "        r_t_shape = [1, 2, 3, 0, 4, 5]\n",
        "    elif num_dims == 4:\n",
        "        votes_t_shape = [3, 0, 1, 2]\n",
        "        r_t_shape = [1, 2, 3, 0]\n",
        "    else:\n",
        "        raise NotImplementedError('Not implemented')\n",
        "\n",
        "    votes_trans = tf.transpose(votes, votes_t_shape)\n",
        "    _, _, _, height, width, caps = votes_trans.get_shape()\n",
        "\n",
        "    def _body(i, logits, activations):\n",
        "        \"\"\"Routing while loop.\"\"\"\n",
        "        # route: [batch, input_dim, output_dim, ...]\n",
        "        a,b,c,d,e = logits.get_shape()\n",
        "        a = logit_shape[0]\n",
        "        b = logit_shape[1]\n",
        "        c = logit_shape[2]\n",
        "        d = logit_shape[3]\n",
        "        e = logit_shape[4]\n",
        "        print(logit_shape)\n",
        "        logit_temp = tf.reshape(logits, [a,b,-1])\n",
        "        route_temp = tf.nn.softmax(logit_temp, dim=-1)\n",
        "        route = tf.reshape(route_temp, [a, b, c, d, e])\n",
        "        preactivate_unrolled = route * votes_trans\n",
        "        preact_trans = tf.transpose(preactivate_unrolled, r_t_shape)\n",
        "        preactivate = tf.reduce_sum(preact_trans, axis=1) + biases\n",
        "        # activation = _squash(preactivate)\n",
        "        activation = squash(preactivate, axis=[-1, -2, -3])\n",
        "        activations = activations.write(i, activation)\n",
        "\n",
        "        act_3d = K.expand_dims(activation, 1)\n",
        "        tile_shape = np.ones(num_dims, dtype=np.int32).tolist()\n",
        "        tile_shape[1] = input_dim\n",
        "        act_replicated = tf.tile(act_3d, tile_shape)\n",
        "        distances = tf.reduce_sum(votes * act_replicated, axis=3)\n",
        "        logits += distances\n",
        "        return (i + 1, logits, activations)\n",
        "\n",
        "    activations = tf.TensorArray(\n",
        "        dtype=tf.float32, size=num_routing, clear_after_read=False)\n",
        "    logits = tf.fill(logit_shape, 0.0)\n",
        "\n",
        "    i = tf.constant(0, dtype=tf.int32)\n",
        "    _, logits, activations = tf.while_loop(\n",
        "        lambda i, logits, activations: i < num_routing,\n",
        "        _body,\n",
        "        loop_vars=[i, logits, activations],\n",
        "        swap_memory=True)\n",
        "    a = K.cast(activations.read(num_routing - 1), dtype='float32')\n",
        "    return K.cast(activations.read(num_routing - 1), dtype='float32')\n",
        "\n",
        "class CapsuleLayer(layers.Layer):\n",
        "\n",
        "    def __init__(self, num_capsule, dim_capsule, channels, routings=3,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.channels = channels\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
        "        self.input_num_capsule = input_shape[1]\n",
        "        self.input_dim_capsule = input_shape[2]\n",
        "\n",
        "        if(self.channels != 0):\n",
        "            assert int(self.input_num_capsule / self.channels) / (self.input_num_capsule / self.channels) == 1, \"error\"\n",
        "            self.W = self.add_weight(shape=[self.num_capsule, self.channels,\n",
        "                                            self.dim_capsule, self.input_dim_capsule],\n",
        "                                     initializer=self.kernel_initializer,\n",
        "                                     name='W')\n",
        "\n",
        "            self.B = self.add_weight(shape=[self.num_capsule, self.dim_capsule],\n",
        "                                     initializer=self.kernel_initializer,\n",
        "                                     name='B')\n",
        "        else:\n",
        "            self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n",
        "                                            self.dim_capsule, self.input_dim_capsule],\n",
        "                                     initializer=self.kernel_initializer,\n",
        "                                     name='W')\n",
        "            self.B = self.add_weight(shape=[self.num_capsule, self.dim_capsule],\n",
        "                                     initializer=self.kernel_initializer,\n",
        "                                     name='B')\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "\n",
        "        inputs_expand = K.expand_dims(inputs, 1)\n",
        "        inputs_tiled = K.tile(inputs_expand, [1, self.num_capsule, 1, 1])\n",
        "\n",
        "        if(self.channels != 0):\n",
        "            W2 = K.repeat_elements(self.W, int(self.input_num_capsule / self.channels), 1)\n",
        "        else:\n",
        "            W2 = self.W\n",
        "        inputs_hat = K.map_fn(lambda x: own_batch_dot(x, W2, [2, 3]), elems=inputs_tiled)\n",
        "\n",
        "        # Routing algorithm\n",
        "        b = tf.zeros(shape=[K.shape(inputs_hat)[0], self.num_capsule, self.input_num_capsule])\n",
        "\n",
        "        assert self.routings > 0, 'The routings should be > 0.'\n",
        "        for i in range(self.routings):\n",
        "            c = tf.nn.softmax(b, dim=1)\n",
        "            outputs = squash(own_batch_dot(c, inputs_hat, [2, 2]) + self.B)  # [None, 10, 16]\n",
        "\n",
        "            if i < self.routings - 1:\n",
        "                b += own_batch_dot(outputs, inputs_hat, [2, 3])\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
        "def squash(vectors, axis=-1):\n",
        "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
        "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm)\n",
        "    return scale * vectors\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCLFaraVZI3j"
      },
      "source": [
        "from load_datasets import *\n",
        "(x_train, y_train), (x_test, y_test) = load_fmnist()\n",
        "train(model=model, data=((x_train, y_train), (x_test, y_test)), hard_training=False, args=args)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}