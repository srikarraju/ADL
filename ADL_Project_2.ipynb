{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1yMrYAWmHts6WWCb-xoNLt1clc4ZTEgJb",
      "authorship_tag": "ABX9TyMTEE9AHY0LHWLZH18DxHf+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srikarraju/ADL/blob/main/ADL_Project_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkwaESx62IlF"
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iva9aXN7ueg9",
        "outputId": "c7c9817a-721c-46b5-af67-0e1a25d3c4bd"
      },
      "source": [
        "from keras.datasets.mnist import load_data\n",
        "\n",
        "(trainX, trainY), (testX, testY) = load_data()\n",
        "\n",
        "print('Train', trainX.shape, trainY.shape)\n",
        "print('Test', testX.shape, testY.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Train (60000, 28, 28) (60000,)\n",
            "Test (10000, 28, 28) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNj_dGvWMi-S",
        "outputId": "23653349-31c6-4c00-9781-8489aee63718"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "trainX_Gan ,testX_Gan, trainY_Gan, testY_Gan = train_test_split(trainX,trainY,train_size = 10000,random_state = 10)\n",
        "class_count = np.zeros(10,dtype=int)\n",
        "for i in range(len(trainY_Gan)):\n",
        "  class_count[trainY_Gan[i]] += 1\n",
        "print(class_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 989 1167  942  993  992  870  996 1056  994 1001]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fubf_EfnwyYt"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D,Dropout,Flatten,Softmax,LeakyReLU,Dense\n",
        "from keras.optimizers import Adam\n",
        "def mnist_classifier(in_shape=(28,28,1)):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(64, (3,3), strides=(2, 2), padding='same', input_shape=in_shape))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Dropout(0.4))\n",
        "\tmodel.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Dropout(0.4))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(10,activation='softmax'))\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjkJhCUiy0-l"
      },
      "source": [
        "import numpy as np\n",
        "def make_samples_3D(arr):\n",
        "\tX = np.expand_dims(arr, axis=-1)\n",
        "\tX = X.astype('float32')\n",
        "\tX = X / 255.0\n",
        "\treturn X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX3e3K2Qx5zs",
        "outputId": "a10444e3-4ff4-4b1f-c102-b76d2d55a141"
      },
      "source": [
        "trainX_Gan_new = make_samples_3D(trainX_Gan)\n",
        "print(trainX_Gan_new.shape)\n",
        "model = mnist_classifier()\n",
        "print(model.summary())\n",
        "#model.fit(trainX_Gan_new,trainY_Gan,epochs =3)\n",
        "\n",
        "testX_new = make_samples_3D(testX)\n",
        "print(testX_new.shape)\n",
        "#print(model.evaluate(testX_new,testY))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 28, 28, 1)\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 64)        640       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                31370     \n",
            "=================================================================\n",
            "Total params: 68,938\n",
            "Trainable params: 68,938\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "(10000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLw79-263k1Y"
      },
      "source": [
        "## Building a GAN\n",
        "def define_discriminator(in_shape=(28,28,1)):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(64, (3,3), strides=(2, 2), padding='same', input_shape=in_shape))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Dropout(0.4))\n",
        "\tmodel.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Dropout(0.4))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# compile model\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A0ClgPz4BeZ"
      },
      "source": [
        "from numpy.random import rand,randint, randn\n",
        "def generate_real_samples(dataset, n_samples):\n",
        "\t# choose random instances\n",
        "\tix = randint(0, dataset.shape[0], n_samples)\n",
        "\t# retrieve selected images\n",
        "\tX = dataset[ix]\n",
        "\t# generate 'real' class labels (1)\n",
        "\ty = np.ones((n_samples, 1))\n",
        "\treturn X, y\n",
        "\n",
        "def generate_fake_samples(n_samples):\n",
        "\t# generate uniform random numbers in [0,1]\n",
        "\tX = rand(28 * 28 * n_samples)\n",
        "\t# reshape into a batch of grayscale images\n",
        "\tX = X.reshape((n_samples, 28, 28, 1))\n",
        "\t# generate 'fake' class labels (0)\n",
        "\ty = np.zeros((n_samples, 1))\n",
        "\treturn X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5ZA9qSG4Kwp"
      },
      "source": [
        "def train_discriminator(model, dataset, n_iter=100, n_batch=256):\n",
        "\thalf_batch = int(n_batch / 2)\n",
        "\t# manually enumerate epochs\n",
        "\tfor i in range(n_iter):\n",
        "\t\t# get randomly selected 'real' samples\n",
        "\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "\t\t# update discriminator on real samples\n",
        "\t\t_, real_acc = model.train_on_batch(X_real, y_real)\n",
        "\t\t# generate 'fake' examples\n",
        "\t\tX_fake, y_fake = generate_fake_samples(half_batch)\n",
        "\t\t# update discriminator on fake samples\n",
        "\t\t_, fake_acc = model.train_on_batch(X_fake, y_fake)\n",
        "\t\t# summarize performance\n",
        "\t\tprint('>%d real=%.0f%% fake=%.0f%%' % (i+1, real_acc*100, fake_acc*100))\n",
        "#model_discriminator = define_discriminator()\n",
        "#train_discriminator(model_discriminator,trainX_Gan_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM7KiFiy5SAl"
      },
      "source": [
        "from keras.layers import Reshape, Conv2DTranspose, Conv2D\n",
        "from keras.utils.vis_utils import plot_model\n",
        "def define_generator(latent_dim):\n",
        "\tmodel = Sequential()\n",
        "\t# foundation for 7x7 image\n",
        "\tn_nodes = 128 * 7 * 7\n",
        "\tmodel.add(Dense(n_nodes, input_dim=latent_dim))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Reshape((7, 7, 128)))\n",
        "\t# upsample to 14x14\n",
        "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t# upsample to 28x28\n",
        "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Conv2D(1, (7,7), activation='sigmoid', padding='same'))\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEmtQnAo6-yt"
      },
      "source": [
        "def generate_latent_points(latent_dim, n_samples):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
        "\treturn x_input\n",
        "def generate_samples_from_generator(g_model, latent_dim, n_samples):\n",
        "\t# generate points in latent space\n",
        "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
        "\t# predict outputs\n",
        "\tX = g_model.predict(x_input)\n",
        "\t# create 'fake' class labels (0)\n",
        "\ty = np.zeros((n_samples, 1))\n",
        "\treturn X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXYlPmX_7UsM"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "latent_dim = 100\n",
        "model = define_generator(latent_dim)\n",
        "n_samples = 25\n",
        "X, _ = generate_samples_from_generator(model, latent_dim, n_samples)\n",
        "\n",
        "for i in range(n_samples):\n",
        "\tpyplot.subplot(5, 5, 1 + i)\n",
        "\tpyplot.axis('off')\n",
        "\tpyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtR6hllq9-V4"
      },
      "source": [
        "label3_samples = []\n",
        "for i in range(len(trainY_Gan)):\n",
        "  if trainY_Gan[i]==3:\n",
        "    label3_samples.append(trainX_Gan_new[i])\n",
        "label3_samples = np.asarray(label3_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziLiHp2T8G6B"
      },
      "source": [
        "def define_gan(g_model, d_model):\n",
        "\td_model.trainable = False\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(g_model)\n",
        "\tmodel.add(d_model)\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVJyAt9Y9rCW"
      },
      "source": [
        "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=256):\n",
        "\tbat_per_epo = int(dataset.shape[0] / n_batch)\n",
        "\thalf_batch = int(n_batch / 2)\n",
        "\tfor i in range(n_epochs):\n",
        "\t\tfor j in range(bat_per_epo):\n",
        "\t\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "\t\t\tX_fake, y_fake = generate_samples_from_generator(g_model, latent_dim, half_batch)\n",
        "\t\t\tX, y = np.vstack((X_real, X_fake)), np.vstack((y_real, y_fake))\n",
        "\t\t\td_loss, _ = d_model.train_on_batch(X, y)\n",
        "\t\t\tX_gan = generate_latent_points(latent_dim, n_batch)\n",
        "\t\t\ty_gan = np.ones((n_batch, 1))\n",
        "\t\t\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
        "\t\t\tprint('>%d, %d/%d, d=%.3f, g=%.3f' % (i+1, j+1, bat_per_epo, d_loss, g_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSRCxoar8XDD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75b1abee-2bd3-47d8-a298-6bb2a28eed96"
      },
      "source": [
        "latent_dim = 100\n",
        "d_model = define_discriminator()\n",
        "g_model = define_generator(latent_dim)\n",
        "\n",
        "gan_model = define_gan(g_model, d_model)\n",
        "#gan_model.summary()\n",
        "#plot_model(gan_model, to_file='gan_plot.png', show_shapes=True, show_layer_names=True)\n",
        "train(g_model, d_model, gan_model, label3_samples, latent_dim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">1, 1/5, d=0.693, g=0.718\n",
            ">1, 2/5, d=0.687, g=0.732\n",
            ">1, 3/5, d=0.679, g=0.745\n",
            ">1, 4/5, d=0.669, g=0.761\n",
            ">1, 5/5, d=0.665, g=0.772\n",
            ">2, 1/5, d=0.656, g=0.786\n",
            ">2, 2/5, d=0.648, g=0.802\n",
            ">2, 3/5, d=0.647, g=0.809\n",
            ">2, 4/5, d=0.642, g=0.813\n",
            ">2, 5/5, d=0.639, g=0.809\n",
            ">3, 1/5, d=0.641, g=0.797\n",
            ">3, 2/5, d=0.645, g=0.782\n",
            ">3, 3/5, d=0.644, g=0.762\n",
            ">3, 4/5, d=0.650, g=0.743\n",
            ">3, 5/5, d=0.648, g=0.727\n",
            ">4, 1/5, d=0.645, g=0.717\n",
            ">4, 2/5, d=0.647, g=0.711\n",
            ">4, 3/5, d=0.639, g=0.707\n",
            ">4, 4/5, d=0.636, g=0.704\n",
            ">4, 5/5, d=0.627, g=0.702\n",
            ">5, 1/5, d=0.623, g=0.702\n",
            ">5, 2/5, d=0.615, g=0.701\n",
            ">5, 3/5, d=0.607, g=0.700\n",
            ">5, 4/5, d=0.600, g=0.701\n",
            ">5, 5/5, d=0.591, g=0.701\n",
            ">6, 1/5, d=0.587, g=0.701\n",
            ">6, 2/5, d=0.581, g=0.701\n",
            ">6, 3/5, d=0.567, g=0.702\n",
            ">6, 4/5, d=0.559, g=0.702\n",
            ">6, 5/5, d=0.555, g=0.703\n",
            ">7, 1/5, d=0.545, g=0.704\n",
            ">7, 2/5, d=0.538, g=0.705\n",
            ">7, 3/5, d=0.522, g=0.706\n",
            ">7, 4/5, d=0.521, g=0.707\n",
            ">7, 5/5, d=0.511, g=0.708\n",
            ">8, 1/5, d=0.502, g=0.710\n",
            ">8, 2/5, d=0.495, g=0.712\n",
            ">8, 3/5, d=0.485, g=0.713\n",
            ">8, 4/5, d=0.476, g=0.715\n",
            ">8, 5/5, d=0.467, g=0.717\n",
            ">9, 1/5, d=0.454, g=0.720\n",
            ">9, 2/5, d=0.452, g=0.723\n",
            ">9, 3/5, d=0.442, g=0.725\n",
            ">9, 4/5, d=0.435, g=0.729\n",
            ">9, 5/5, d=0.431, g=0.732\n",
            ">10, 1/5, d=0.418, g=0.737\n",
            ">10, 2/5, d=0.414, g=0.741\n",
            ">10, 3/5, d=0.418, g=0.746\n",
            ">10, 4/5, d=0.396, g=0.752\n",
            ">10, 5/5, d=0.394, g=0.758\n",
            ">11, 1/5, d=0.382, g=0.764\n",
            ">11, 2/5, d=0.381, g=0.772\n",
            ">11, 3/5, d=0.379, g=0.780\n",
            ">11, 4/5, d=0.368, g=0.791\n",
            ">11, 5/5, d=0.364, g=0.800\n",
            ">12, 1/5, d=0.356, g=0.810\n",
            ">12, 2/5, d=0.347, g=0.825\n",
            ">12, 3/5, d=0.345, g=0.840\n",
            ">12, 4/5, d=0.331, g=0.853\n",
            ">12, 5/5, d=0.327, g=0.870\n",
            ">13, 1/5, d=0.312, g=0.889\n",
            ">13, 2/5, d=0.305, g=0.908\n",
            ">13, 3/5, d=0.300, g=0.930\n",
            ">13, 4/5, d=0.294, g=0.953\n",
            ">13, 5/5, d=0.280, g=0.980\n",
            ">14, 1/5, d=0.272, g=1.006\n",
            ">14, 2/5, d=0.268, g=1.039\n",
            ">14, 3/5, d=0.256, g=1.071\n",
            ">14, 4/5, d=0.251, g=1.103\n",
            ">14, 5/5, d=0.242, g=1.139\n",
            ">15, 1/5, d=0.231, g=1.177\n",
            ">15, 2/5, d=0.218, g=1.219\n",
            ">15, 3/5, d=0.207, g=1.265\n",
            ">15, 4/5, d=0.203, g=1.308\n",
            ">15, 5/5, d=0.188, g=1.357\n",
            ">16, 1/5, d=0.182, g=1.409\n",
            ">16, 2/5, d=0.170, g=1.455\n",
            ">16, 3/5, d=0.159, g=1.512\n",
            ">16, 4/5, d=0.156, g=1.562\n",
            ">16, 5/5, d=0.147, g=1.623\n",
            ">17, 1/5, d=0.141, g=1.677\n",
            ">17, 2/5, d=0.134, g=1.731\n",
            ">17, 3/5, d=0.124, g=1.794\n",
            ">17, 4/5, d=0.120, g=1.847\n",
            ">17, 5/5, d=0.114, g=1.902\n",
            ">18, 1/5, d=0.107, g=1.956\n",
            ">18, 2/5, d=0.108, g=2.012\n",
            ">18, 3/5, d=0.096, g=2.066\n",
            ">18, 4/5, d=0.095, g=2.115\n",
            ">18, 5/5, d=0.090, g=2.172\n",
            ">19, 1/5, d=0.088, g=2.225\n",
            ">19, 2/5, d=0.083, g=2.256\n",
            ">19, 3/5, d=0.084, g=2.206\n",
            ">19, 4/5, d=0.264, g=1.259\n",
            ">19, 5/5, d=1.930, g=0.066\n",
            ">20, 1/5, d=2.568, g=0.035\n",
            ">20, 2/5, d=2.130, g=0.081\n",
            ">20, 3/5, d=1.572, g=0.242\n",
            ">20, 4/5, d=1.055, g=0.602\n",
            ">20, 5/5, d=0.670, g=1.135\n",
            ">21, 1/5, d=0.482, g=1.598\n",
            ">21, 2/5, d=0.425, g=1.979\n",
            ">21, 3/5, d=0.417, g=2.081\n",
            ">21, 4/5, d=0.359, g=2.132\n",
            ">21, 5/5, d=0.349, g=2.063\n",
            ">22, 1/5, d=0.326, g=1.976\n",
            ">22, 2/5, d=0.325, g=1.863\n",
            ">22, 3/5, d=0.302, g=1.748\n",
            ">22, 4/5, d=0.305, g=1.651\n",
            ">22, 5/5, d=0.325, g=1.505\n",
            ">23, 1/5, d=0.326, g=1.399\n",
            ">23, 2/5, d=0.336, g=1.307\n",
            ">23, 3/5, d=0.337, g=1.265\n",
            ">23, 4/5, d=0.355, g=1.246\n",
            ">23, 5/5, d=0.349, g=1.212\n",
            ">24, 1/5, d=0.346, g=1.218\n",
            ">24, 2/5, d=0.357, g=1.205\n",
            ">24, 3/5, d=0.347, g=1.219\n",
            ">24, 4/5, d=0.342, g=1.211\n",
            ">24, 5/5, d=0.324, g=1.171\n",
            ">25, 1/5, d=0.364, g=1.174\n",
            ">25, 2/5, d=0.372, g=1.155\n",
            ">25, 3/5, d=0.364, g=1.132\n",
            ">25, 4/5, d=0.369, g=1.121\n",
            ">25, 5/5, d=0.383, g=1.134\n",
            ">26, 1/5, d=0.369, g=1.131\n",
            ">26, 2/5, d=0.380, g=1.142\n",
            ">26, 3/5, d=0.378, g=1.148\n",
            ">26, 4/5, d=0.374, g=1.195\n",
            ">26, 5/5, d=0.387, g=1.178\n",
            ">27, 1/5, d=0.372, g=1.187\n",
            ">27, 2/5, d=0.343, g=1.226\n",
            ">27, 3/5, d=0.388, g=1.223\n",
            ">27, 4/5, d=0.351, g=1.244\n",
            ">27, 5/5, d=0.365, g=1.246\n",
            ">28, 1/5, d=0.348, g=1.262\n",
            ">28, 2/5, d=0.351, g=1.257\n",
            ">28, 3/5, d=0.353, g=1.275\n",
            ">28, 4/5, d=0.320, g=1.291\n",
            ">28, 5/5, d=0.326, g=1.321\n",
            ">29, 1/5, d=0.339, g=1.329\n",
            ">29, 2/5, d=0.318, g=1.365\n",
            ">29, 3/5, d=0.312, g=1.368\n",
            ">29, 4/5, d=0.303, g=1.377\n",
            ">29, 5/5, d=0.329, g=1.382\n",
            ">30, 1/5, d=0.317, g=1.395\n",
            ">30, 2/5, d=0.290, g=1.409\n",
            ">30, 3/5, d=0.334, g=1.388\n",
            ">30, 4/5, d=0.292, g=1.408\n",
            ">30, 5/5, d=0.286, g=1.430\n",
            ">31, 1/5, d=0.273, g=1.459\n",
            ">31, 2/5, d=0.314, g=1.470\n",
            ">31, 3/5, d=0.300, g=1.445\n",
            ">31, 4/5, d=0.292, g=1.455\n",
            ">31, 5/5, d=0.304, g=1.457\n",
            ">32, 1/5, d=0.304, g=1.416\n",
            ">32, 2/5, d=0.277, g=1.375\n",
            ">32, 3/5, d=0.308, g=1.316\n",
            ">32, 4/5, d=0.322, g=1.280\n",
            ">32, 5/5, d=0.337, g=1.261\n",
            ">33, 1/5, d=0.343, g=1.296\n",
            ">33, 2/5, d=0.335, g=1.322\n",
            ">33, 3/5, d=0.334, g=1.333\n",
            ">33, 4/5, d=0.312, g=1.360\n",
            ">33, 5/5, d=0.325, g=1.399\n",
            ">34, 1/5, d=0.309, g=1.428\n",
            ">34, 2/5, d=0.322, g=1.446\n",
            ">34, 3/5, d=0.344, g=1.434\n",
            ">34, 4/5, d=0.309, g=1.444\n",
            ">34, 5/5, d=0.337, g=1.429\n",
            ">35, 1/5, d=0.280, g=1.442\n",
            ">35, 2/5, d=0.288, g=1.476\n",
            ">35, 3/5, d=0.312, g=1.496\n",
            ">35, 4/5, d=0.273, g=1.511\n",
            ">35, 5/5, d=0.277, g=1.561\n",
            ">36, 1/5, d=0.271, g=1.561\n",
            ">36, 2/5, d=0.316, g=1.580\n",
            ">36, 3/5, d=0.320, g=1.553\n",
            ">36, 4/5, d=0.321, g=1.536\n",
            ">36, 5/5, d=0.314, g=1.518\n",
            ">37, 1/5, d=0.296, g=1.501\n",
            ">37, 2/5, d=0.264, g=1.546\n",
            ">37, 3/5, d=0.261, g=1.568\n",
            ">37, 4/5, d=0.304, g=1.598\n",
            ">37, 5/5, d=0.337, g=1.580\n",
            ">38, 1/5, d=0.292, g=1.560\n",
            ">38, 2/5, d=0.245, g=1.583\n",
            ">38, 3/5, d=0.291, g=1.602\n",
            ">38, 4/5, d=0.278, g=1.593\n",
            ">38, 5/5, d=0.238, g=1.631\n",
            ">39, 1/5, d=0.287, g=1.649\n",
            ">39, 2/5, d=0.340, g=1.624\n",
            ">39, 3/5, d=0.298, g=1.600\n",
            ">39, 4/5, d=0.292, g=1.604\n",
            ">39, 5/5, d=0.272, g=1.606\n",
            ">40, 1/5, d=0.293, g=1.617\n",
            ">40, 2/5, d=0.286, g=1.626\n",
            ">40, 3/5, d=0.312, g=1.612\n",
            ">40, 4/5, d=0.325, g=1.596\n",
            ">40, 5/5, d=0.273, g=1.573\n",
            ">41, 1/5, d=0.262, g=1.595\n",
            ">41, 2/5, d=0.299, g=1.618\n",
            ">41, 3/5, d=0.289, g=1.630\n",
            ">41, 4/5, d=0.254, g=1.638\n",
            ">41, 5/5, d=0.276, g=1.643\n",
            ">42, 1/5, d=0.270, g=1.675\n",
            ">42, 2/5, d=0.313, g=1.655\n",
            ">42, 3/5, d=0.294, g=1.640\n",
            ">42, 4/5, d=0.302, g=1.628\n",
            ">42, 5/5, d=0.267, g=1.644\n",
            ">43, 1/5, d=0.267, g=1.621\n",
            ">43, 2/5, d=0.317, g=1.631\n",
            ">43, 3/5, d=0.297, g=1.626\n",
            ">43, 4/5, d=0.299, g=1.614\n",
            ">43, 5/5, d=0.249, g=1.658\n",
            ">44, 1/5, d=0.255, g=1.686\n",
            ">44, 2/5, d=0.313, g=1.677\n",
            ">44, 3/5, d=0.289, g=1.673\n",
            ">44, 4/5, d=0.253, g=1.692\n",
            ">44, 5/5, d=0.255, g=1.718\n",
            ">45, 1/5, d=0.314, g=1.705\n",
            ">45, 2/5, d=0.275, g=1.673\n",
            ">45, 3/5, d=0.307, g=1.649\n",
            ">45, 4/5, d=0.266, g=1.637\n",
            ">45, 5/5, d=0.320, g=1.606\n",
            ">46, 1/5, d=0.243, g=1.648\n",
            ">46, 2/5, d=0.322, g=1.651\n",
            ">46, 3/5, d=0.375, g=1.593\n",
            ">46, 4/5, d=0.266, g=1.566\n",
            ">46, 5/5, d=0.265, g=1.593\n",
            ">47, 1/5, d=0.294, g=1.606\n",
            ">47, 2/5, d=0.305, g=1.642\n",
            ">47, 3/5, d=0.288, g=1.641\n",
            ">47, 4/5, d=0.261, g=1.639\n",
            ">47, 5/5, d=0.292, g=1.666\n",
            ">48, 1/5, d=0.288, g=1.652\n",
            ">48, 2/5, d=0.297, g=1.637\n",
            ">48, 3/5, d=0.302, g=1.612\n",
            ">48, 4/5, d=0.314, g=1.603\n",
            ">48, 5/5, d=0.301, g=1.622\n",
            ">49, 1/5, d=0.321, g=1.613\n",
            ">49, 2/5, d=0.289, g=1.620\n",
            ">49, 3/5, d=0.247, g=1.636\n",
            ">49, 4/5, d=0.248, g=1.685\n",
            ">49, 5/5, d=0.282, g=1.708\n",
            ">50, 1/5, d=0.280, g=1.706\n",
            ">50, 2/5, d=0.261, g=1.739\n",
            ">50, 3/5, d=0.326, g=1.704\n",
            ">50, 4/5, d=0.305, g=1.677\n",
            ">50, 5/5, d=0.301, g=1.655\n",
            ">51, 1/5, d=0.272, g=1.673\n",
            ">51, 2/5, d=0.269, g=1.681\n",
            ">51, 3/5, d=0.287, g=1.724\n",
            ">51, 4/5, d=0.239, g=1.749\n",
            ">51, 5/5, d=0.230, g=1.796\n",
            ">52, 1/5, d=0.286, g=1.798\n",
            ">52, 2/5, d=0.276, g=1.770\n",
            ">52, 3/5, d=0.266, g=1.774\n",
            ">52, 4/5, d=0.298, g=1.753\n",
            ">52, 5/5, d=0.281, g=1.756\n",
            ">53, 1/5, d=0.270, g=1.734\n",
            ">53, 2/5, d=0.265, g=1.732\n",
            ">53, 3/5, d=0.246, g=1.774\n",
            ">53, 4/5, d=0.274, g=1.793\n",
            ">53, 5/5, d=0.314, g=1.749\n",
            ">54, 1/5, d=0.278, g=1.730\n",
            ">54, 2/5, d=0.265, g=1.760\n",
            ">54, 3/5, d=0.267, g=1.744\n",
            ">54, 4/5, d=0.292, g=1.716\n",
            ">54, 5/5, d=0.246, g=1.728\n",
            ">55, 1/5, d=0.311, g=1.723\n",
            ">55, 2/5, d=0.287, g=1.712\n",
            ">55, 3/5, d=0.281, g=1.689\n",
            ">55, 4/5, d=0.286, g=1.688\n",
            ">55, 5/5, d=0.316, g=1.664\n",
            ">56, 1/5, d=0.279, g=1.661\n",
            ">56, 2/5, d=0.282, g=1.699\n",
            ">56, 3/5, d=0.233, g=1.722\n",
            ">56, 4/5, d=0.263, g=1.770\n",
            ">56, 5/5, d=0.294, g=1.751\n",
            ">57, 1/5, d=0.243, g=1.786\n",
            ">57, 2/5, d=0.224, g=1.844\n",
            ">57, 3/5, d=0.251, g=1.845\n",
            ">57, 4/5, d=0.239, g=1.865\n",
            ">57, 5/5, d=0.241, g=1.888\n",
            ">58, 1/5, d=0.266, g=1.886\n",
            ">58, 2/5, d=0.233, g=1.926\n",
            ">58, 3/5, d=0.251, g=1.879\n",
            ">58, 4/5, d=0.263, g=1.845\n",
            ">58, 5/5, d=0.277, g=1.830\n",
            ">59, 1/5, d=0.342, g=1.713\n",
            ">59, 2/5, d=0.251, g=1.681\n",
            ">59, 3/5, d=0.255, g=1.713\n",
            ">59, 4/5, d=0.254, g=1.720\n",
            ">59, 5/5, d=0.223, g=1.804\n",
            ">60, 1/5, d=0.277, g=1.805\n",
            ">60, 2/5, d=0.277, g=1.782\n",
            ">60, 3/5, d=0.250, g=1.804\n",
            ">60, 4/5, d=0.225, g=1.813\n",
            ">60, 5/5, d=0.269, g=1.841\n",
            ">61, 1/5, d=0.278, g=1.811\n",
            ">61, 2/5, d=0.295, g=1.782\n",
            ">61, 3/5, d=0.255, g=1.793\n",
            ">61, 4/5, d=0.292, g=1.750\n",
            ">61, 5/5, d=0.273, g=1.743\n",
            ">62, 1/5, d=0.287, g=1.725\n",
            ">62, 2/5, d=0.286, g=1.724\n",
            ">62, 3/5, d=0.281, g=1.709\n",
            ">62, 4/5, d=0.238, g=1.767\n",
            ">62, 5/5, d=0.301, g=1.781\n",
            ">63, 1/5, d=0.293, g=1.718\n",
            ">63, 2/5, d=0.241, g=1.721\n",
            ">63, 3/5, d=0.338, g=1.682\n",
            ">63, 4/5, d=0.487, g=1.516\n",
            ">63, 5/5, d=0.560, g=1.392\n",
            ">64, 1/5, d=0.482, g=1.586\n",
            ">64, 2/5, d=0.562, g=1.727\n",
            ">64, 3/5, d=0.567, g=1.790\n",
            ">64, 4/5, d=0.456, g=1.758\n",
            ">64, 5/5, d=0.457, g=1.734\n",
            ">65, 1/5, d=0.522, g=1.713\n",
            ">65, 2/5, d=0.452, g=1.757\n",
            ">65, 3/5, d=0.279, g=1.875\n",
            ">65, 4/5, d=0.448, g=1.893\n",
            ">65, 5/5, d=0.436, g=1.912\n",
            ">66, 1/5, d=0.323, g=1.937\n",
            ">66, 2/5, d=0.351, g=1.912\n",
            ">66, 3/5, d=0.330, g=1.967\n",
            ">66, 4/5, d=0.355, g=1.942\n",
            ">66, 5/5, d=0.339, g=1.905\n",
            ">67, 1/5, d=0.367, g=1.856\n",
            ">67, 2/5, d=0.374, g=1.820\n",
            ">67, 3/5, d=0.366, g=1.886\n",
            ">67, 4/5, d=0.453, g=1.813\n",
            ">67, 5/5, d=0.673, g=1.537\n",
            ">68, 1/5, d=1.483, g=0.964\n",
            ">68, 2/5, d=2.951, g=0.093\n",
            ">68, 3/5, d=1.862, g=1.484\n",
            ">68, 4/5, d=1.620, g=1.221\n",
            ">68, 5/5, d=1.547, g=0.971\n",
            ">69, 1/5, d=1.634, g=0.637\n",
            ">69, 2/5, d=1.633, g=0.513\n",
            ">69, 3/5, d=1.506, g=0.489\n",
            ">69, 4/5, d=1.548, g=0.503\n",
            ">69, 5/5, d=1.341, g=0.513\n",
            ">70, 1/5, d=1.360, g=0.561\n",
            ">70, 2/5, d=1.228, g=0.570\n",
            ">70, 3/5, d=1.172, g=0.619\n",
            ">70, 4/5, d=1.143, g=0.649\n",
            ">70, 5/5, d=1.102, g=0.671\n",
            ">71, 1/5, d=1.056, g=0.696\n",
            ">71, 2/5, d=0.973, g=0.710\n",
            ">71, 3/5, d=0.937, g=0.710\n",
            ">71, 4/5, d=0.912, g=0.714\n",
            ">71, 5/5, d=0.863, g=0.786\n",
            ">72, 1/5, d=0.851, g=0.778\n",
            ">72, 2/5, d=0.809, g=0.780\n",
            ">72, 3/5, d=0.802, g=0.801\n",
            ">72, 4/5, d=0.783, g=0.832\n",
            ">72, 5/5, d=0.778, g=0.809\n",
            ">73, 1/5, d=0.721, g=0.832\n",
            ">73, 2/5, d=0.721, g=0.859\n",
            ">73, 3/5, d=0.688, g=0.886\n",
            ">73, 4/5, d=0.670, g=0.916\n",
            ">73, 5/5, d=0.683, g=0.925\n",
            ">74, 1/5, d=0.658, g=0.922\n",
            ">74, 2/5, d=0.634, g=0.940\n",
            ">74, 3/5, d=0.654, g=0.943\n",
            ">74, 4/5, d=0.634, g=0.960\n",
            ">74, 5/5, d=0.618, g=0.958\n",
            ">75, 1/5, d=0.658, g=0.961\n",
            ">75, 2/5, d=0.645, g=0.941\n",
            ">75, 3/5, d=0.613, g=0.964\n",
            ">75, 4/5, d=0.667, g=0.991\n",
            ">75, 5/5, d=0.673, g=0.930\n",
            ">76, 1/5, d=0.626, g=0.924\n",
            ">76, 2/5, d=0.654, g=0.926\n",
            ">76, 3/5, d=0.646, g=0.950\n",
            ">76, 4/5, d=0.673, g=0.929\n",
            ">76, 5/5, d=0.672, g=0.923\n",
            ">77, 1/5, d=0.656, g=0.950\n",
            ">77, 2/5, d=0.683, g=0.910\n",
            ">77, 3/5, d=0.638, g=0.896\n",
            ">77, 4/5, d=0.666, g=0.912\n",
            ">77, 5/5, d=0.650, g=0.867\n",
            ">78, 1/5, d=0.672, g=0.887\n",
            ">78, 2/5, d=0.647, g=0.883\n",
            ">78, 3/5, d=0.648, g=0.887\n",
            ">78, 4/5, d=0.644, g=0.904\n",
            ">78, 5/5, d=0.631, g=0.923\n",
            ">79, 1/5, d=0.644, g=0.897\n",
            ">79, 2/5, d=0.673, g=0.799\n",
            ">79, 3/5, d=0.671, g=0.810\n",
            ">79, 4/5, d=0.669, g=0.784\n",
            ">79, 5/5, d=0.672, g=0.738\n",
            ">80, 1/5, d=0.632, g=0.764\n",
            ">80, 2/5, d=0.707, g=0.753\n",
            ">80, 3/5, d=0.659, g=0.762\n",
            ">80, 4/5, d=0.679, g=0.751\n",
            ">80, 5/5, d=0.678, g=0.758\n",
            ">81, 1/5, d=0.695, g=0.749\n",
            ">81, 2/5, d=0.689, g=0.713\n",
            ">81, 3/5, d=0.694, g=0.720\n",
            ">81, 4/5, d=0.704, g=0.733\n",
            ">81, 5/5, d=0.705, g=0.724\n",
            ">82, 1/5, d=0.703, g=0.736\n",
            ">82, 2/5, d=0.732, g=0.721\n",
            ">82, 3/5, d=0.725, g=0.716\n",
            ">82, 4/5, d=0.718, g=0.725\n",
            ">82, 5/5, d=0.752, g=0.697\n",
            ">83, 1/5, d=0.757, g=0.694\n",
            ">83, 2/5, d=0.746, g=0.673\n",
            ">83, 3/5, d=0.761, g=0.678\n",
            ">83, 4/5, d=0.774, g=0.656\n",
            ">83, 5/5, d=0.809, g=0.622\n",
            ">84, 1/5, d=0.774, g=0.628\n",
            ">84, 2/5, d=0.807, g=0.629\n",
            ">84, 3/5, d=0.811, g=0.638\n",
            ">84, 4/5, d=0.811, g=0.614\n",
            ">84, 5/5, d=0.840, g=0.644\n",
            ">85, 1/5, d=0.820, g=0.651\n",
            ">85, 2/5, d=0.835, g=0.639\n",
            ">85, 3/5, d=0.856, g=0.609\n",
            ">85, 4/5, d=0.857, g=0.625\n",
            ">85, 5/5, d=0.840, g=0.656\n",
            ">86, 1/5, d=0.845, g=0.635\n",
            ">86, 2/5, d=0.821, g=0.658\n",
            ">86, 3/5, d=0.826, g=0.682\n",
            ">86, 4/5, d=0.827, g=0.687\n",
            ">86, 5/5, d=0.823, g=0.670\n",
            ">87, 1/5, d=0.826, g=0.666\n",
            ">87, 2/5, d=0.777, g=0.675\n",
            ">87, 3/5, d=0.830, g=0.674\n",
            ">87, 4/5, d=0.840, g=0.642\n",
            ">87, 5/5, d=0.812, g=0.693\n",
            ">88, 1/5, d=0.829, g=0.711\n",
            ">88, 2/5, d=0.786, g=0.712\n",
            ">88, 3/5, d=0.799, g=0.703\n",
            ">88, 4/5, d=0.822, g=0.736\n",
            ">88, 5/5, d=0.795, g=0.734\n",
            ">89, 1/5, d=0.807, g=0.752\n",
            ">89, 2/5, d=0.788, g=0.733\n",
            ">89, 3/5, d=0.786, g=0.754\n",
            ">89, 4/5, d=0.767, g=0.775\n",
            ">89, 5/5, d=0.771, g=0.777\n",
            ">90, 1/5, d=0.760, g=0.769\n",
            ">90, 2/5, d=0.774, g=0.763\n",
            ">90, 3/5, d=0.758, g=0.753\n",
            ">90, 4/5, d=0.760, g=0.752\n",
            ">90, 5/5, d=0.765, g=0.738\n",
            ">91, 1/5, d=0.761, g=0.708\n",
            ">91, 2/5, d=0.768, g=0.697\n",
            ">91, 3/5, d=0.763, g=0.699\n",
            ">91, 4/5, d=0.748, g=0.702\n",
            ">91, 5/5, d=0.746, g=0.714\n",
            ">92, 1/5, d=0.747, g=0.699\n",
            ">92, 2/5, d=0.732, g=0.720\n",
            ">92, 3/5, d=0.728, g=0.736\n",
            ">92, 4/5, d=0.734, g=0.744\n",
            ">92, 5/5, d=0.724, g=0.772\n",
            ">93, 1/5, d=0.713, g=0.769\n",
            ">93, 2/5, d=0.707, g=0.790\n",
            ">93, 3/5, d=0.697, g=0.803\n",
            ">93, 4/5, d=0.694, g=0.809\n",
            ">93, 5/5, d=0.668, g=0.828\n",
            ">94, 1/5, d=0.681, g=0.840\n",
            ">94, 2/5, d=0.671, g=0.841\n",
            ">94, 3/5, d=0.671, g=0.822\n",
            ">94, 4/5, d=0.669, g=0.843\n",
            ">94, 5/5, d=0.653, g=0.842\n",
            ">95, 1/5, d=0.671, g=0.849\n",
            ">95, 2/5, d=0.650, g=0.846\n",
            ">95, 3/5, d=0.652, g=0.844\n",
            ">95, 4/5, d=0.661, g=0.831\n",
            ">95, 5/5, d=0.635, g=0.820\n",
            ">96, 1/5, d=0.620, g=0.815\n",
            ">96, 2/5, d=0.646, g=0.813\n",
            ">96, 3/5, d=0.631, g=0.807\n",
            ">96, 4/5, d=0.644, g=0.778\n",
            ">96, 5/5, d=0.633, g=0.774\n",
            ">97, 1/5, d=0.631, g=0.789\n",
            ">97, 2/5, d=0.642, g=0.754\n",
            ">97, 3/5, d=0.644, g=0.753\n",
            ">97, 4/5, d=0.648, g=0.759\n",
            ">97, 5/5, d=0.635, g=0.744\n",
            ">98, 1/5, d=0.641, g=0.740\n",
            ">98, 2/5, d=0.630, g=0.742\n",
            ">98, 3/5, d=0.642, g=0.735\n",
            ">98, 4/5, d=0.639, g=0.752\n",
            ">98, 5/5, d=0.637, g=0.742\n",
            ">99, 1/5, d=0.654, g=0.739\n",
            ">99, 2/5, d=0.640, g=0.740\n",
            ">99, 3/5, d=0.639, g=0.742\n",
            ">99, 4/5, d=0.663, g=0.740\n",
            ">99, 5/5, d=0.651, g=0.756\n",
            ">100, 1/5, d=0.638, g=0.770\n",
            ">100, 2/5, d=0.661, g=0.768\n",
            ">100, 3/5, d=0.658, g=0.786\n",
            ">100, 4/5, d=0.648, g=0.820\n",
            ">100, 5/5, d=0.651, g=0.826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3Rvdmby92Po",
        "outputId": "24e786bd-dacf-491d-bb39-e1fcc56e40bd"
      },
      "source": [
        "filename = '/content/drive/MyDrive/Colab Notebooks/ADL/ADL_2/models/generator_model_label7'\n",
        "g_model.save(filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/ADL/ADL_2/models/generator_model_label7/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwa_2zRIN2xL"
      },
      "source": [
        "def generate_latent_points(latent_dim, n_samples):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
        "\treturn x_input\n",
        "\n",
        "# create and save a plot of generated images (reversed grayscale)\n",
        "def save_plot(examples, n):\n",
        "\t# plot images\n",
        "\tfor i in range(n * n):\n",
        "\t\t# define subplot\n",
        "\t\tpyplot.subplot(n, n, 1 + i)\n",
        "\t\t# turn off axis\n",
        "\t\tpyplot.axis('off')\n",
        "\t\t# plot raw pixel data\n",
        "\t\tpyplot.imshow(examples[i, :, :, 0], cmap='gray_r')\n",
        "\tpyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "yuUBdJ6BNy4e",
        "outputId": "50bda795-1432-45d5-d44c-1f7a27480843"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "#model = load_model('/content/drive/MyDrive/Colab Notebooks/ADL/ADL_2/models/generator_model_label9')\n",
        "latent_points = generate_latent_points(100, 11)\n",
        "X = g_model.predict(latent_points)\n",
        "pyplot.imshow(X[0, :, :, 0], cmap='gray_r')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f6b6c22bc88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QXFUgTRMBNf",
        "outputId": "af116958-8d39-4cb2-e4ee-97a723e5c089"
      },
      "source": [
        "label3_samples = np.concatenate((label3_samples,X),axis=0)\n",
        "print(label3_samples.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1293, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhlTGY-7SK8d"
      },
      "source": [
        "syn_train_set = []\n",
        "syn_set_labels = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7m2wv-3SW8a"
      },
      "source": [
        "#syn_train_set = X\n",
        "syn_train_set = np.concatenate((syn_train_set,X),axis=0)\n",
        "syn_set_labels += [9]*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rVyFOBLSrtk",
        "outputId": "8da8b72a-026d-4a7e-e4c4-8148817a089a"
      },
      "source": [
        "print(len(syn_train_set))\n",
        "print(len(syn_set_labels))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILbPlkE_Ou65",
        "outputId": "af1c4202-1060-4b5c-9baf-7417d4da5d66"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "trainX_final ,testX_final, trainY_final, testY_final = train_test_split(testX_Gan,testY_Gan,train_size = 1000,random_state = 10)\n",
        "class_count = np.zeros(10,dtype=int)\n",
        "for i in range(len(trainY_final)):\n",
        "  class_count[trainY_final[i]] += 1\n",
        "print(class_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 972 1121 1034 1063  983  898  976 1046  953  954]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SbeJqLfQrEn",
        "outputId": "2e92a5c1-005d-42e1-b102-feb5f048e241"
      },
      "source": [
        "trainX_final_new = make_samples_3D(syn_train_set)\n",
        "print(trainX_final_new.shape)\n",
        "model = mnist_classifier()\n",
        "print(model.summary())\n",
        "syn_set_labels_ = np.asarray(syn_set_labels)\n",
        "model.fit(trainX_final_new,syn_set_labels_,epochs =2)\n",
        "\n",
        "testX_new = make_samples_3D(testX)\n",
        "print(testX_new.shape)\n",
        "print(model.evaluate(testX_new,testY))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 28, 28, 1, 1)\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_5 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 79,510\n",
            "Trainable params: 79,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/2\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1026 - accuracy: 0.3133\n",
            "Epoch 2/2\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6352 - accuracy: 0.9181\n",
            "(10000, 28, 28, 1)\n",
            "313/313 [==============================] - 0s 936us/step - loss: 270.4607 - accuracy: 0.4546\n",
            "[270.4606628417969, 0.4546000063419342]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEIhXTgTnfQc",
        "outputId": "2d4ad1a7-b6e9-492a-dd7c-38e7865a1800"
      },
      "source": [
        "trainX_Gan_,testX_Gan_, trainY_Gan_, testY_Gan_ = train_test_split(testX_Gan,testY_Gan,train_size = 1000,random_state = 10)\n",
        "trainX_Gan_new = make_samples_3D(trainX_Gan_)\n",
        "print(trainX_Gan_new.shape)\n",
        "model = mnist_classifier()\n",
        "print(model.summary())\n",
        "model.fit(trainX_Gan_new,trainY_Gan_,epochs =2)\n",
        "\n",
        "testX_new = make_samples_3D(testX)\n",
        "print(testX_new.shape)\n",
        "print(model.evaluate(testX_new,testY))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 28, 28, 1)\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 13, 13, 32)        320       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 5408)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                54090     \n",
            "=================================================================\n",
            "Total params: 54,410\n",
            "Trainable params: 54,410\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/2\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1.6677 - accuracy: 0.5046\n",
            "Epoch 2/2\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.5565 - accuracy: 0.8231\n",
            "(10000, 28, 28, 1)\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5794 - accuracy: 0.8381\n",
            "[0.579399824142456, 0.838100016117096]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8UfQm44RyFh"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D,Dropout,Flatten,Softmax,LeakyReLU,Dense\n",
        "from keras.optimizers import Adam\n",
        "def mnist_classifier(in_shape=(28,28,1)):\n",
        "\tmodel = Sequential()\n",
        "\t#model.add(Flatten(input_shape=in_shape))\n",
        "\tmodel.add(Conv2D(32, (3,3), strides=(2, 2), input_shape=in_shape))\n",
        "\tmodel.add(LeakyReLU(alpha=0.4))\n",
        "\tmodel.add(Dropout(0.6))\n",
        "\t# model.add(Conv2D(64, (3,3), strides=(2, 2)))\n",
        "\t# model.add(LeakyReLU(alpha=0.2))\n",
        "\t# model.add(Dropout(0.6))\n",
        "\tmodel.add(Flatten())\n",
        "\t#model.add(Dense(100,activation='relu'))\n",
        "\tmodel.add(Dense(10,activation='softmax'))\n",
        "\topt = Adam(lr=0.03, beta_1=0.5)\n",
        "\tmodel.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}